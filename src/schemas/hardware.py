"""
Hardware detection schemas per SPEC_v3 Section 4.5.

Defines HardwareProfile and related dataclasses for platform-specific
hardware detection and tier classification.

NEW in Phase 1 - does not replace existing code.
See: docs/MIGRATION_PROTOCOL.md Section 3
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional, List


class PlatformType(Enum):
    """Supported platform types."""
    APPLE_SILICON = "apple_silicon"
    WINDOWS_NVIDIA = "windows_nvidia"
    LINUX_NVIDIA = "linux_nvidia"
    WSL2_NVIDIA = "wsl2_nvidia"
    LINUX_ROCM = "linux_rocm"
    CPU_ONLY = "cpu_only"
    UNKNOWN = "unknown"


class HardwareTier(Enum):
    """
    Hardware capability tiers per SPEC_v3 Section 4.5.

    Determines recommended models and capabilities.
    """
    WORKSTATION = "workstation"   # 48GB+ - All FP16, training, multi-model
    PROFESSIONAL = "professional" # 16-24GB - All FP8, HunyuanVideo
    PROSUMER = "prosumer"         # 12GB - Flux FP8, Wan 14B Q5
    CONSUMER = "consumer"         # 8GB - SDXL, Flux Q4-Q5, Wan 5B
    ENTRY = "entry"               # 4-6GB - SD1.5, SDXL Q4
    MINIMAL = "minimal"           # <4GB or CPU only


class ThermalState(Enum):
    """GPU thermal state classification."""
    NORMAL = "normal"
    WARNING = "warning"     # Approaching throttle (82-84C)
    CRITICAL = "critical"   # Active throttling (85C+)
    UNKNOWN = "unknown"


@dataclass
class HardwareProfile:
    """
    Comprehensive hardware profile per SPEC_v3 Section 4.5.

    Generated by platform-specific detectors. Used by recommendation
    engine for constraint satisfaction and model filtering.
    """
    # Core identification
    platform: PlatformType
    gpu_vendor: str  # "nvidia", "apple", "amd", "none"
    gpu_name: str    # Human-readable GPU name

    # Memory
    vram_gb: float              # Effective VRAM (with ceiling applied for Apple Silicon)
    ram_gb: float               # System RAM
    unified_memory: bool = False  # True for Apple Silicon

    # Compute capabilities (NVIDIA-specific)
    compute_capability: Optional[float] = None  # e.g., 8.9 for RTX 4090
    supports_fp8: bool = False
    supports_bf16: bool = False
    supports_tf32: bool = False
    flash_attention_available: bool = False

    # Apple Silicon-specific
    mps_available: bool = False
    memory_bandwidth_gbps: Optional[float] = None
    chip_variant: Optional[str] = None  # "M3 Max", "M4 Pro", etc.

    # NVIDIA multi-GPU
    gpu_count: int = 1
    nvlink_available: bool = False

    # AMD ROCm-specific
    rocm_version: Optional[str] = None
    gfx_version: Optional[str] = None  # e.g., "gfx1100"
    officially_supported: bool = True  # False for RDNA2 workaround
    hsa_override_required: Optional[str] = None  # HSA_OVERRIDE_GFX_VERSION value

    # Derived
    tier: HardwareTier = HardwareTier.MINIMAL
    thermal_state: ThermalState = ThermalState.UNKNOWN

    # Warnings/constraints for UI display
    warnings: List[str] = field(default_factory=list)
    platform_constraints: List[str] = field(default_factory=list)

    def __post_init__(self):
        """Auto-calculate tier if not set."""
        if self.tier == HardwareTier.MINIMAL:
            self.tier = self._calculate_tier()
        self._apply_platform_constraints()

    def _calculate_tier(self) -> HardwareTier:
        """
        Calculate hardware tier based on effective VRAM.
        Per SPEC_v3 Section 4.5.
        """
        vram = self.vram_gb

        if vram >= 48:
            return HardwareTier.WORKSTATION
        elif vram >= 16:
            return HardwareTier.PROFESSIONAL
        elif vram >= 12:
            return HardwareTier.PROSUMER
        elif vram >= 8:
            return HardwareTier.CONSUMER
        elif vram >= 4:
            return HardwareTier.ENTRY
        else:
            return HardwareTier.MINIMAL

    def _apply_platform_constraints(self):
        """Apply platform-specific constraints per SPEC_v3 Section 4.2-4.4."""
        if self.platform == PlatformType.APPLE_SILICON:
            self.platform_constraints = [
                "GGUF K-quants not supported (use Q4_0, Q5_0, Q8_0)",
                "FP8 quantization not available",
                "Flash Attention not available",
                "BF16 not hardware-accelerated",
            ]
            if self.vram_gb < 12:
                self.platform_constraints.append(
                    "HunyuanVideo excluded (~16 min/clip impractical)"
                )

        elif self.platform == PlatformType.LINUX_ROCM:
            self.platform_constraints = [
                "Marked as experimental",
                "Some CUDA-specific ComfyUI nodes unavailable",
            ]
            if not self.officially_supported:
                self.platform_constraints.append(
                    f"RDNA2 workaround required: {self.hsa_override_required}"
                )

        elif self.compute_capability and self.compute_capability < 8.0:
            self.platform_constraints = [
                "Flash Attention unavailable (Turing architecture)",
                "BF16 not supported - using FP16",
            ]

    @property
    def can_run_fp8(self) -> bool:
        """Check if hardware supports FP8 precision."""
        return self.supports_fp8

    @property
    def can_run_hunyuan(self) -> bool:
        """Check if HunyuanVideo is practical on this hardware."""
        # Excluded for Apple Silicon < Professional tier
        if self.platform == PlatformType.APPLE_SILICON:
            return self.tier in (HardwareTier.WORKSTATION, HardwareTier.PROFESSIONAL)
        # NVIDIA/AMD: Need at least 12GB
        return self.vram_gb >= 12

    @property
    def allowed_gguf_quants(self) -> List[str]:
        """Return allowed GGUF quantization types for this platform."""
        if self.platform == PlatformType.APPLE_SILICON:
            # K-quants crash on MPS
            return ["Q4_0", "Q5_0", "Q8_0"]
        # All quantizations available on NVIDIA/AMD
        return ["Q4_0", "Q4_K_M", "Q5_0", "Q5_K_M", "Q6_K", "Q8_0"]


# Note: DetectionError was moved to src/services/hardware/base.py as an Exception class
# (DetectionFailedError) since errors should be raised, not returned as data.
